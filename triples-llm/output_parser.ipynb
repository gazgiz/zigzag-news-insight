{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "class Triple(BaseModel):\n",
    "    subject : str = Field(description=\"subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.llms import LMFormatEnforcer\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain.output_parsers import PydanticOutputParser,OutputFixingParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, GPT2TokenizerFast,  AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:10<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "auto_config = AutoConfig.from_pretrained(\"beomi/Llama-3-Open-Ko-8B\", trust_remote_code = True)\n",
    "\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"beomi/Llama-3-Open-Ko-8B\",\n",
    "    trust_remote_code = True, \n",
    "    device_map=\"auto\",\n",
    "    cache_dir = '/data',\n",
    "    torch_dtype=torch.float16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/Llama-3-Open-Ko-8B\", use_fast=True, trust_remote_code = True, padding_side='left', truncation_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left' # to prevent errors with FA\n",
    "tokenizer.truncation_side = 'left' # to prevent cutting off last generation\n",
    "\n",
    "llm = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=128, pad_token_id=model.config.eos_token_id,\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {'role':'system', 'content': \"You are a helpful, respectful and honest assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"{instruction}\\n{format_instructions}\\ntext:```{query}```\\n\"},    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = PromptTemplate(  ## TODO : change template automatically\n",
    "    template=tokenizer.apply_chat_template(chat, tokenize=False),\n",
    "    input_variables=[\"instruction\", \"format_instructions\", \"query\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"My name is dokyoon.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_and_model = PROMPT_TEMPLATE | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = PROMPT_TEMPLATE.format(\n",
    "        instruction=\"Please extract the subject from the following text.\", \n",
    "        format_instructions=Triple.schema(), #  parser.get_format_instructions(), \n",
    "        query=data)\n",
    "    \n",
    "lm_format_enforcer = LMFormatEnforcer(\n",
    "    json_schema=Triple.schema(), pipeline=llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lm_format_enforcer.generate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for generation in results.generations:\n",
    "    parsed_output = parser.invoke(generation[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject='from'\n"
     ]
    }
   ],
   "source": [
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabularize",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
